---
title: "Some designs have badly posed questions but design diagnosis can alert you to the problem"
output: html_document
# bibliography: ../bib/book.bib 
bibliography: ../../bib/book.bib # use this line comment the above
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

```{r, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 1000
b_sims <- 1000
```

## Some designs have badly posed questions but design diagnosis can alert you to the problem

An obvious requirement of a good research design is that the question it seeks to answer does in fact *have* an answer, at least under plausible models of the world. 

Interestingly, we can sometimes get quite far along a research path without being conscious that the questions we ask do not have answers and the answers we get are answering different questions. 

Fortunately, computers complain when they are asked to answer badly posted questions.

How could a question not have an answer? Answerless questions can arise when inquiries depend on variables that do not exist or are undefined for some units. 

Consider an audit experiment that seeks to assess the effects of an email from a Latino name (versus a White name) on *whether* and *how well* election officials respond to requests for information. For example, do they use a positive or negative tone. These questions seem reasonable enough. The problem, however, is that if there are officials who don't send responses, tone is undefined. More subtly, if there is an official that does send an email but would not have sent it in a different treatment condition, then tone is undefined for one of their potential outcomes. 

### A design with sometimes undefined outcomes

Here are the key parts to the design:

- **M**odel: The model has two outcome variables, $R_i$ and $Y_i$. $R_i$ stands for "response" and is equal to 1 if a response is sent, and 0 otherwise. $Y_i$ is the tone of the response and is normally distributed when it is defined. $Z_i$ is the treatment and equals 1 if the email is sent using a Latino name and 0 otherwise. The table below shows the potential outcomes for four possible types of subjects, depending on the potential outcomes of $R_i$.
    
     * *A* types respond if and only if they are *not* treated
     * *B* types respond if and only if they are treated   
     * *C* types never respond, regardless of treatment 
     * *D* types always respond regardless of treatment 

\indent The table also includes columns for the potential outcomes of $Y_i$, showing which potential outcome subjects would express depending on their type. The key thing to note is that for the *A*, *B*, and *C* types, the effect of treatment on $Y_i$ is *undefined* because messages never sent have no tone. The last (and very important) feature of our model is that the outcomes $Y_i$ are possibly correlated with subject type. Even though both $E[Y_i(1) | \text{Type} = D]$ and $E[Y_i(1) | \text{Type} = B]$ exist, there's no reason to expect that they are the same. 

| Type | $R_i(0)$ | $R_i(1)$ | $E(Y_i(0))$  | $E(Y_i(1))$ |
| ---- | -------- | -------- | ------------ | -------- |
| A    | 1        | 0        | -1         | NA       |
| B    | 0        | 1        | NA           | -1 |
| C    | 0        | 0        | NA           | NA       |
| D    | 1        | 1        | 0          | 1 |

Table: Causal Types 

- **I**nquiry:  We have two inquiries. The first is straightforward: $E[R_i(1) - R_i(0)]$ is the Average Treatment Effect on response. The second inquiry is the undefined inquiry that does not have an answer: $E[Y_i(1) - Y_i(0)]$. We will also consider a third inquiry, which *is* defined: $E[Y_i(1) - Y_i(0) | \mathrm{Type} = D]$, which is the average effect of treatment on tone among $D$ types.

- **D**ata strategy: The data strategy will be to use complete random assignment to assign 250 of 500 units to treatment.

- **A**nswer strategy: We'll try to answer all three inquiries with the difference-in-means estimator.

This design can be declared formally like this:

```{r}
# Model -------------------------------------------------------------------
population <- declare_population(
  N = 500, noise = rnorm(N),
  type = sample(1:4, N, replace = TRUE, prob = c(0, 1/3, 1/3, 1/3)),
  A = type == 1, B = type == 2, C = type == 3, D = type == 4)

pos <- declare_potential_outcomes(
  R_Z_0 = A|D,   # A and D types report in the control condition
  R_Z_1 = B|D,   # B and D types report in the treatment condition
  Y_Z_0 = ifelse(R_Z_0, 0*D - 1*A + noise, NA),
  Y_Z_1 = ifelse(R_Z_1, 1*D - 1*B + noise, NA))

# Inquiry -----------------------------------------------------------------
estimands <- declare_estimand( ATE_R = mean(R_Z_1 - R_Z_0),
                               ATE_Y = mean(Y_Z_1 - Y_Z_0),
                               ATE_Y_for_Ds = mean(Y_Z_1[D] - Y_Z_0[D]))

# Data Strategy -----------------------------------------------------------
assignment <- declare_assignment()
reveal     <- declare_reveal(outcome_variables = c("R", "Y"))

# Answer Strategy ---------------------------------------------------------
est1 <- declare_estimator(R ~ Z, estimand = "ATE_R", label = "DIM_R")
est2 <- declare_estimator(Y ~ Z, estimand = list("ATE_Y", "ATE_Y_for_Ds"), 
                           label = "DIM_Y")

# Design ------------------------------------------------------------------
design <- population + pos + estimands + assignment + reveal + est1 + est2
```

### What we find from diagnosis

Here is a diagnosis of this design:

```{r, eval = do_diagnosis}
diagnosis <- diagnose_design(design, sims = sims, bootstrap_sims = b_sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("Check_that_your_estimands_are_well_defined"), "/diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, path = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```

```{r, echo = FALSE}
kable(reshape_diagnosis(diagnosis)[,-2])
```

We learn three things from the design diagnosis. 

* First, as expected, our experiment is unbiased for the average treatment effect on response.

* Second,  our second inquiry, as well as our diagnostics for it, are undefined. The diagnosis tells us that our definition of potential outcomes produces a definition problem for the estimand. Note that some diagnosands  are defined, including power. These are diagnosands that depend only on the answer strategy and not on the estimand.

* The third estimand -- the average effects for the $D$ types -- *is* defined but our estimates are biased. The reason for this is that we cannot tell from the data which types are the $D$ types: we are not conditioning on the correct subset. Indeed, we are unable to condition on the correct subset. If a subject responds in the treatment group, we don't know if she is an $B$ or a $D$ type; in the control group, we can't tell if a responder is an $A$ or a $D$ type. Our difference-in-means estimator of the ATE on $Y$ among $D$s will be off whenever $D$s have different outcomes from $A$s and $B$s.

There are some solutions to this problem. In some cases, the problem might be resolved by changing the inquiry. Closely related estimands can often be defined, perhaps by redefining $Y$ (e.g., emails never sent have a tone of zero). Some redefinitions of the problem, as in the one we examine above, require estimating effects for unobserved subgroups which is a difficult challenge.

### Other instances of this problem

This kind of problem is surprisingly common. Here are three more instances of the problem:

1. $Y$ is the decision to vote Democrat ($Y=1$) or Republican ($Y=0$), $R$ is the decision to turn out to vote and $Z$ is a campaign message. The decision to vote may depend on treatment but if subjects do not vote then $Y$ is undefined.
2. $Y$ is the weight of infants, $R$ is whether a child is born and $Z$ is a maternal health intervention. Fertility may depend on treatment but the weight of unborn (possibly never conceived) babies is not defined.
2. $Y$ is the charity to whom contributions are made during fundraising and $R$ is whether anything is contributed and $Z$ is an encouragement to contribute. The identity of beneficiaries is not defined if there are no contributions. 

All of these problem exhibit a form of post treatment bias (see section **Post treatment bias**) but the issue goes beyond picking the right estimator. Our problem here is conceptual: the effect of treatment on the outcome just doesn't exist for some subjects.

### Keep thinking

Some puzzles:

1. The amount of bias on the third estimand depends on both the distribution of types and the correlation of types with the potential outcomes of *Y*. Modify the declaration so that the estimator of the effect on *Y* is unbiased, changing only the distribution of types. Repeat the exercise, changing only the relation between the types and  the potential outcomes of $Y$.  

2. Try approaching the problem by redefining the inquiry, seeking to assess the effect of treatment on the share of responses with positive tone. 
