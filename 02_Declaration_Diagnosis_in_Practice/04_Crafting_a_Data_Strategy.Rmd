---
title: "Crafting a data strategy"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Crafting a data strategy

<!-- make sure to rename the section title below -->

```{r crafting_a_data_strategy, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

The data strategy is what researchers do in the world in order to collect information about it. Depending on the design, it could include decisions about any or or all of the following: how to sample or select cases, how to assign treatments, or how to measure outcomes. 

These choices apply to all kinds of research. In experimental research, a large focus is given to the assignment of treatments. How many treatment conditions should there be? Should we use a simple coin flip to decide who recieves treatment, or should we use a more complicated strategy like blocking? Experimenters are of course also very concerned with sampling and measurement procedures, but it is the random assignment to treatments that make experiments distinctive among research designs. 

Quantitative descriptive research, on the other hand, often has an inquiry like the population average of some outcome variable. Since the goal here is to draw inferences about a population on the basis of a sample, we need to pay special attention to the procedure by which units are selected into the sample. We might use a random sampling procedure in order to generate a design-based justification for generalizing from samples to population. Nonrandom sampling procedures are also possible: convenience sampling, respondent-driven sampling, and snowball sampling are all data strategies that do not include an explictly random component.

Once we have selected units into the sample, we need to measure them in some way. The tools we use to measure are a critical part of the data strategy. For many social scientific studies, a main way we collect information is through surveys. A huge methodological literature on survey administration has developed to help guide researchers who have to design questionnaires. Bad survey questions yield distorted or noisy responses. They can be distored if responses are systematically biased away from the true latent target the question is designed to measure, in which case the question has low *validity*. They can be noisy if (hypothetically) you would obtain different answers each time you asked the same person the same question, in which case the question has low *reliability*. 

Beyond surveys, we might use administrative data to collect outcomes. The concerns about validity and reliability do not disappear once we move out of the survey environment. The information that shows up in an administrative database is itself the result of many human decisions, each of which has the possibility of increasing or decreasing the distance between the measurement and the thing to be measured.

Researchers have to choose good sampling, assignment, and measurement techniques that, when combined and applied to the world, will produce information that is ready for analysis. We will discuss answer strategies -- the set of analysis choices about what to do with the data once it's collected -- in the next chapter. The data and answer strategies are of course intimately interconnected. How you analyze data depends deeply on how it was collected *and* how you collect data depends just as deeply on how you plan to analyze it. For the moment, we are thinking through the many choices we might make as part of the data strategy, but of course they will have to be considered in concert with the answer strategy in any applied research design setting.

The data strategy is a *set of procedures* that result in a dataset. It is important to keep these two concepts straight. If you apply data strategy $D$, it produces dataset $d$. The data $d$ is the *result* of the data strategy $D$. We say $d$ is "the" result of $D$, since when we apply the data strategy to the world, we only do so once and we obtain the data that we obtain. But when we are crafting a data strategy, we have to think about the many datasets that the data strategy *could have* produced. Some of the datasets might be really excellent. For example, in good datasets, we achieve good covariate balance across the treatment and control groups. Or we might draw a sample whose distribution of observable characteristics looks really similar to the population. But some of the datasets might be worse: because of the vagaries of randomization, the particular realizations of the random assignment or random sampling might more more or less balanced. We do not have to settle for data strategies that can produce worse datasets! We want to choose data strategy $D$ that is likely to result in a high-quality dataset $d$.

## Choosing a sampling procedure

- simple, complete, stratified, clustered, stratified and clustered
- weighted sampling (over/undersampling)
- quota sampling

## Choosing an assignment procedure

- simple, complete, blocked, clustered, blocked and clustered
- point restricted randomization
- no assignment procedure at all
- multiple arms

## Choosing a measurement procedure

- Should this be where we do the first bit of distinction between latent and observed

- more T (david McKenzie).  How frequently to measure.  Andy - arguing against intermediate measurement?

- multiple measurements of Y.  make a scale



## 

- just downloading the data.  Did you offload the data strategy
- possibly ambiguous where the data strategy ends and the analysis strategy ends.
- SOMEone did parts of the datastrategy






